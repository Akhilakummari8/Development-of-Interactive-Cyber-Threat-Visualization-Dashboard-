provide the Website change detection to create a module provide code and generate dataset 
import requests
from bs4 import BeautifulSoup
import hashlib
import pandas as pd
from datetime import datetime
import os

# Folder to store website snapshots
SNAPSHOT_DIR = "snapshots"
DATASET_FILE = "change_detection_dataset.csv"

os.makedirs(SNAPSHOT_DIR, exist_ok=True)

def fetch_website_text(url):
    # Add a User-Agent header to mimic a web browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status() # Raise an exception for HTTP errors
    soup = BeautifulSoup(response.text, "html.parser")

    # Remove script & style content
    for script in soup(["script", "style"]):
        script.decompose()

    text = soup.get_text(separator=" ", strip=True)
    return text

def generate_hash(content):
    return hashlib.sha256(content.encode("utf-8")).hexdigest()

def detect_change(url):
    try:
        content = fetch_website_text(url)
        content_hash = generate_hash(content)

        snapshot_file = os.path.join(SNAPSHOT_DIR, content_hash + ".txt")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        change_detected = not os.path.exists(snapshot_file)

        if change_detected:
            with open(snapshot_file, "w", encoding="utf-8") as f:
                f.write(content)

        return {
            "url": url,
            "timestamp": timestamp,
            "content_hash": content_hash,
            "change_detected": change_detected
        }
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return {
            "url": url,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "content_hash": "error",
            "change_detected": True # Treat fetch error as a change/issue
        }

def update_dataset(record):
    df = pd.DataFrame([record])

    if os.path.exists(DATASET_FILE):
        df_existing = pd.read_csv(DATASET_FILE)
        df = pd.concat([df_existing, df], ignore_index=True)

    df.to_csv(DATASET_FILE, index=False)

if __name__ == "__main__":
    urls = [
        "https://example.com",
        "https://www.wikipedia.org"

    ]

    for url in urls:
        record = detect_change(url)
        update_dataset(record)
        print(f"Checked: {url} | Change Detected: {record['change_detected']}")

OUTPUT
Checked: https://example.com | Change Detected: False
Checked: https://www.wikipedia.org | Change Detected: True

